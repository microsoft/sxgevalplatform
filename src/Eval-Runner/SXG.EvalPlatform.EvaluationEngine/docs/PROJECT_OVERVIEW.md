# SXG Evaluation Platform - Evaluation Engine

## ğŸ“‹ Project Overview

The **SXG Evaluation Platform Evaluation Engine** is a sophisticated, cloud-native Python application designed to run AI agent evaluations at scale. It processes evaluation requests through Azure Storage Queues, executes comprehensive metrics using the Azure AI Evaluation SDK, and provides detailed results for AI system assessment.

### ğŸ¯ Purpose
- **Automated AI Evaluation**: Evaluate AI agents and models using industry-standard metrics
- **Scalable Processing**: Handle multiple evaluation requests concurrently
- **Azure-Native**: Built for Azure cloud deployment with Managed Identity integration
- **Extensible Framework**: Modular architecture supporting custom evaluation metrics

## ğŸ—ï¸ Architecture Overview

### High-Level Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Queue Message â”‚â”€â”€â”€â–¶â”‚  Evaluation      â”‚â”€â”€â”€â–¶â”‚   Results       â”‚
â”‚   (Eval Request)â”‚    â”‚  Engine          â”‚    â”‚   (Blob Storage)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ Azure AI         â”‚
                       â”‚ Evaluation SDK   â”‚
                       â”‚ â€¢ Agentic        â”‚
                       â”‚ â€¢ RAG            â”‚
                       â”‚ â€¢ Safety         â”‚
                       â”‚ â€¢ Similarity     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

1. **ğŸ® Main Application (`main.py`)**
   - Entry point and orchestration
   - Signal handling for graceful shutdown
   - Queue message processing loop

2. **âš™ï¸ Evaluation Engine (`core/evaluation_engine.py`)**
   - Main business logic coordinator
   - Dataset processing
   - Metrics execution management
   - Results aggregation and storage

3. **ğŸ“Š Metrics System (`metrics/`)**
   - Azure AI Evaluation SDK integration
   - 20+ pre-configured evaluators across 4 categories
   - Base classes for consistent implementation  
   - Results standardization with enhanced error handling

4. **â˜ï¸ Azure Services (`services/`)**
   - Queue message handling with Managed Identity and connection pooling
   - Blob storage for results with Managed Identity
   - HTTP client with connection pooling and lifecycle management
   - Success/failure logging to separate queues

5. **âš™ï¸ Configuration (`config/settings.py`)**
   - Environment-aware configuration management
   - Managed Identity support
   - Validation and error handling

## ğŸ“ Project Structure

```
src/eval_runner/
â”œâ”€â”€ ğŸ“„ main.py                          # Application entry point
â”œâ”€â”€ ğŸ“ config/
â”‚   â””â”€â”€ ğŸ“„ settings.py                  # Configuration management
â”œâ”€â”€ ğŸ“ core/
â”‚   â””â”€â”€ ğŸ“„ evaluation_engine.py         # Main evaluation logic
â”œâ”€â”€ ğŸ“ services/
â”‚   â”œâ”€â”€ ğŸ“„ azure_storage.py             # Azure Storage services
â”‚   â”œâ”€â”€ ğŸ“„ evaluation_api_client.py     # External API client
â”‚   â””â”€â”€ ğŸ“„ http_client.py               # HTTP utilities
â”œâ”€â”€ ğŸ“ models/
â”‚   â””â”€â”€ ğŸ“„ eval_models.py               # Data models and DTOs
â”œâ”€â”€ ğŸ“ metrics/
â”‚   â”œâ”€â”€ ğŸ“„ azure_ai_interface.py        # Azure AI SDK interface
â”‚   â”œâ”€â”€ ğŸ“„ base_evaluators.py           # Abstract base classes
â”‚   â”œâ”€â”€ ğŸ“„ evaluation_result.py         # Result models
â”‚   â”œâ”€â”€ ğŸ“ agentic/                     # Intent, Task, Tool evaluators
â”‚   â”œâ”€â”€ ğŸ“ rag/                         # Groundedness, Relevance, etc.
â”‚   â”œâ”€â”€ ğŸ“ risk_and_safety/             # Safety evaluators
â”‚   â””â”€â”€ ğŸ“ text_similarity/             # F1, ROUGE, Similarity
â”œâ”€â”€ ğŸ“„ azure_ai_config.py               # Azure AI SDK configuration
â””â”€â”€ ğŸ“„ exceptions.py                    # Custom exceptions
```

## ğŸ”„ Application Workflow

### 1. **Startup Sequence**
```python
# main.py â†’ EvaluationApp.start()
1. Setup signal handlers (SIGINT, SIGTERM)
2. Initialize queue service with Managed Identity
3. Start listening for queue messages
4. Process messages asynchronously
```

### 2. **Message Processing Flow**
```python
# EvaluationApp._handle_queue_message()
Queue Message â†’ Parse â†’ Validation â†’ Evaluation Engine â†’ Results Storage
```

### 3. **Evaluation Engine Workflow**
```python
# evaluation_engine.py â†’ process_queue_message()
1. Fetch enriched dataset from API
2. Load metrics configuration
3. Initialize Azure AI evaluators
4. Process dataset items in parallel
5. Aggregate results and calculate scores
6. Upload results to blob storage
7. Update status via API
```

## ğŸ“Š Metrics System Details

### Azure AI Evaluation Categories

#### 1. **ğŸ¤– Agentic Evaluators** (`metrics/agentic/`)
- **IntentResolutionEvaluator**: Measures intent understanding accuracy
- **ToolCallAccuracyEvaluator**: Validates tool usage correctness
- **TaskAdherenceEvaluator**: Checks task completion quality

#### 2. **ğŸ“š RAG (Retrieval-Augmented Generation)** (`metrics/rag/`)
- **GroundednessEvaluator**: Assesses factual grounding in context
- **RelevanceEvaluator**: Measures response relevance to query
- **CoherenceEvaluator**: Evaluates logical consistency
- **FluencyEvaluator**: Assesses naturalness and readability

#### 3. **ğŸ›¡ï¸ Risk & Safety** (`metrics/risk_and_safety/`)
- **ViolenceEvaluator**: Detects violent content
- **SexualEvaluator**: Identifies inappropriate sexual content
- **SelfHarmEvaluator**: Flags self-harm references
- **HateUnfairnessEvaluator**: Detects hate speech and bias
- **IndirectAttackEvaluator**: Identifies prompt injection attempts

#### 4. **ğŸ“ˆ Text Similarity** (`metrics/text_similarity/`)
- **SimilarityEvaluator**: Semantic similarity assessment
- **F1ScoreEvaluator**: Precision/recall harmonic mean
- **RougeScoreEvaluator**: Summary evaluation metrics

### Base Evaluator Classes

```python
# base_evaluators.py
BaseAzureEvaluator       # Common interface and lifecycle
â”œâ”€â”€ ModelBasedEvaluator  # LLM-judge evaluators (GPT-based)
â”œâ”€â”€ SafetyEvaluator      # Azure AI Foundry safety evaluators
â””â”€â”€ StatisticalEvaluator # Statistical/computational metrics
```

## ğŸ”§ Configuration System

### Configuration Classes

```python
# settings.py
AppSettings
â”œâ”€â”€ AzureStorageConfig      # Storage account, queues, blobs
â”œâ”€â”€ ApiEndpointsConfig      # External API endpoints
â”œâ”€â”€ ApiKeysConfig           # Authentication keys
â”œâ”€â”€ EvaluationConfig        # Execution parameters
â”œâ”€â”€ AzureOpenAIConfig       # Azure OpenAI for LLM judges
â”œâ”€â”€ AzureAIConfig           # Azure AI Foundry project
â””â”€â”€ LoggingConfig           # Logging levels and formats
```

### Environment Variables Support
```bash
# Azure Storage
AZURE_STORAGE_ACCOUNT_NAME=your-storage-account
AZURE_USE_MANAGED_IDENTITY=true

# API Configuration  
EVAL_API_KEY=your-api-key
EVAL_CONFIG_ENDPOINT=https://your-api.com/config

# Azure AI Services
AZURE_OPENAI_ENDPOINT=https://your-openai.azure.com
AZURE_AI_PROJECT_NAME=your-project
```

## ğŸš€ Deployment Options

### 1. **Azure Container Apps** (Recommended)
```dockerfile
# Uses provided Dockerfile
# Automatic scaling based on queue length
# Managed Identity integration
```

### 2. **Azure Container Instances**
```bash
# Single instance deployment
# Manual scaling
# Cost-effective for low volume
```

### 3. **Local Development**
```bash
# Docker Compose setup
# Connection string fallback
# Hot reload support
```

## ğŸ”’ Security Features

### Managed Identity Integration
- **Storage Access**: Queue and Blob operations without secrets
- **Azure AI Services**: Automatic credential management
- **API Authentication**: Environment-based key management

### Required Azure Permissions
```bash
# Storage Account
- Storage Queue Data Contributor
- Storage Blob Data Contributor

# Azure AI Foundry
- Cognitive Services User
- Azure AI Developer
```

## ğŸ“ˆ Performance Characteristics

### Concurrency Settings
```json
{
  "Evaluation": {
    "MaxParallelPrompts": 10,     // Concurrent dataset items
    "MaxParallelMetrics": 5,      // Concurrent metric evaluations
    "TimeoutSeconds": 300,        // Per-evaluation timeout
    "RetryAttempts": 3            // Failure retry count
  }
}
```

### Scaling Behavior
- **Queue Processing**: Single message at a time per instance
- **Dataset Evaluation**: Parallel processing within evaluation
- **Metric Execution**: Concurrent evaluation across metrics
- **Auto-scaling**: Based on queue length in Azure Container Apps

## ğŸ› ï¸ Development Workflow

### Adding New Metrics
1. Create evaluator class inheriting from appropriate base class
2. Implement required abstract methods
3. Register in `azure_ai_interface.py`
4. Add configuration in `azure_ai_config.py`
5. Update tests and documentation

### Testing Strategy
```bash
# Unit Tests
pytest tests/test_refactored_evaluators.py -v

# Integration Tests
pytest tests/test_azure_storage.py -v

# End-to-end Tests  
python src/test_metrics.py
```

## ğŸ“š Key Files to Understand

### **Start Here** ğŸ“
1. **`README.md`** - This comprehensive overview
2. **`main.py`** - Application entry point and flow
3. **`core/evaluation_engine.py`** - Main business logic

### **Core Architecture**
4. **`config/settings.py`** - Configuration management
5. **`services/azure_storage.py`** - Azure integration
6. **`metrics/azure_ai_interface.py`** - Metrics system

### **Implementation Details**
7. **`models/eval_models.py`** - Data structures
8. **`metrics/base_evaluators.py`** - Evaluation framework
9. **`azure_ai_config.py`** - Azure AI SDK setup

### **Deployment & Operations**
10. **`DEPLOYMENT.md`** - Deployment guide
11. **`AZURE_STORAGE_MANAGED_IDENTITY.md`** - Security setup
12. **`docker-compose.yml`** - Local development

## ğŸ” Troubleshooting

### Common Issues
- **Authentication**: Verify Managed Identity permissions
- **Configuration**: Check environment variable formatting
- **Network**: Ensure Azure service connectivity
- **Performance**: Monitor queue depth and processing times

### Logging and Monitoring
```python
# Structured logging with levels
logger.info("Processing evaluation", extra={
    "eval_run_id": message.eval_run_id,
    "metrics_count": len(metrics)
})
```

---

**Next Steps**: Start with `main.py` to understand the application flow, then explore `core/evaluation_engine.py` for the main business logic. The metrics system in `metrics/` demonstrates the Azure AI SDK integration patterns.